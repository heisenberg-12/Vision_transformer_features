{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T20:59:11.468908Z",
     "start_time": "2023-07-01T20:59:05.367099100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from extractor_sd import load_model, process_features_and_mask, get_mask\n",
    "from utils.utils_correspondence import co_pca, resize, find_nearest_patchs, find_nearest_patchs_replace\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from extractor_dino import ViTExtractor\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import linear_sum_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T20:59:16.685975500Z",
     "start_time": "2023-07-01T20:59:16.684974800Z"
    }
   },
   "outputs": [],
   "source": [
    "MASK = True\n",
    "VER = \"v1-5\"\n",
    "PCA = False\n",
    "CO_PCA = True\n",
    "PCA_DIMS = [256, 256, 256]\n",
    "SIZE =960\n",
    "EDGE_PAD = False\n",
    "\n",
    "FUSE_DINO = 1\n",
    "ONLY_DINO = 0\n",
    "DINOV2 = True\n",
    "MODEL_SIZE = 'base'\n",
    "DRAW_DENSE = 1\n",
    "DRAW_SWAP = 1\n",
    "TEXT_INPUT = False\n",
    "SEED = 42\n",
    "TIMESTEP = 100\n",
    "\n",
    "DIST = 'l2' if FUSE_DINO and not ONLY_DINO else 'cos'\n",
    "if ONLY_DINO:\n",
    "    FUSE_DINO = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-01T20:59:19.397333700Z",
     "start_time": "2023-07-01T20:59:18.789766800Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Panoptic/odise_label_coco_50e.py not available in Model Zoo!",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mmanual_seed(SEED)\n\u001B[1;32m      4\u001B[0m torch\u001B[38;5;241m.\u001B[39mbackends\u001B[38;5;241m.\u001B[39mcudnn\u001B[38;5;241m.\u001B[39mbenchmark \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m model, aug \u001B[38;5;241m=\u001B[39m \u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdiffusion_ver\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mVER\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTIMESTEP\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/mnt/d/Libraries/Documents/Freiburg Informatiks Work/Deep Learning Lab/Vision_transformer_features/src/models/sd_dino/sd_dino/extractor_sd.py:170\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(config_path, seed, diffusion_ver, image_size, num_timesteps, block_indices, decoder_only, encoder_only, resblock_only)\u001B[0m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_model\u001B[39m(config_path\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPanoptic/odise_label_coco_50e.py\u001B[39m\u001B[38;5;124m\"\u001B[39m, seed\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, diffusion_ver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv1-3\u001B[39m\u001B[38;5;124m\"\u001B[39m, image_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1024\u001B[39m, num_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m, block_indices\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m5\u001B[39m,\u001B[38;5;241m8\u001B[39m,\u001B[38;5;241m11\u001B[39m), decoder_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, encoder_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, resblock_only\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m--> 170\u001B[0m     cfg \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_zoo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrained\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    172\u001B[0m     cfg\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mbackbone\u001B[38;5;241m.\u001B[39mfeature_extractor\u001B[38;5;241m.\u001B[39minit_checkpoint \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msd://\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m+\u001B[39mdiffusion_ver\n\u001B[1;32m    173\u001B[0m     cfg\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mbackbone\u001B[38;5;241m.\u001B[39mfeature_extractor\u001B[38;5;241m.\u001B[39msteps \u001B[38;5;241m=\u001B[39m (num_timesteps,)\n",
      "File \u001B[0;32m/mnt/d/Libraries/Documents/Freiburg Informatiks Work/Deep Learning Lab/Vision_transformer_features/src/models/sd_dino/sd_dino/third_party/ODISE/odise/model_zoo/model_zoo.py:110\u001B[0m, in \u001B[0;36mget_config\u001B[0;34m(config_path, trained)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_config\u001B[39m(config_path, trained: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m    Returns a config object for a model in model zoo.\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;124;03m        CfgNode or omegaconf.DictConfig: a config object\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 110\u001B[0m     cfg_file \u001B[38;5;241m=\u001B[39m \u001B[43mget_config_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    111\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m cfg_file\u001B[38;5;241m.\u001B[39mendswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.py\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    112\u001B[0m     cfg \u001B[38;5;241m=\u001B[39m LazyConfig\u001B[38;5;241m.\u001B[39mload(cfg_file)\n",
      "File \u001B[0;32m/mnt/d/Libraries/Documents/Freiburg Informatiks Work/Deep Learning Lab/Vision_transformer_features/src/models/sd_dino/sd_dino/third_party/ODISE/odise/model_zoo/model_zoo.py:92\u001B[0m, in \u001B[0;36mget_config_file\u001B[0;34m(config_path)\u001B[0m\n\u001B[1;32m     88\u001B[0m cfg_file \u001B[38;5;241m=\u001B[39m pkg_resources\u001B[38;5;241m.\u001B[39mresource_filename(\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124modise.model_zoo\u001B[39m\u001B[38;5;124m\"\u001B[39m, os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconfigs\u001B[39m\u001B[38;5;124m\"\u001B[39m, config_path)\n\u001B[1;32m     90\u001B[0m )\n\u001B[1;32m     91\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(cfg_file):\n\u001B[0;32m---> 92\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m not available in Model Zoo!\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(config_path))\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cfg_file\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Panoptic/odise_label_coco_50e.py not available in Model Zoo!"
     ]
    }
   ],
   "source": [
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model, aug = load_model(diffusion_ver=VER, image_size=SIZE, num_timesteps=TIMESTEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_feature(model, aug, save_path, files, category, mask=False, dist='cos', real_size=960):\n",
    "    if type(category) == str:\n",
    "        category = [category]\n",
    "    img_size = 840 if DINOV2 else 244\n",
    "    model_dict={'small':'dinov2_vits14',\n",
    "                'base':'dinov2_vitb14',\n",
    "                'large':'dinov2_vitl14',\n",
    "                'giant':'dinov2_vitg14'}\n",
    "    \n",
    "    model_type = model_dict[MODEL_SIZE] if DINOV2 else 'dino_vits8'\n",
    "    layer = 11 if DINOV2 else 9\n",
    "    if 'l' in model_type:\n",
    "        layer = 23\n",
    "    elif 'g' in model_type:\n",
    "        layer = 39\n",
    "    facet = 'token' if DINOV2 else 'key'\n",
    "    stride = 14 if DINOV2 else 4\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    # indiactor = 'v2' if DINOV2 else 'v1'\n",
    "    # model_size = model_type.split('vit')[-1]\n",
    "    extractor = ViTExtractor(model_type, stride, device=device)\n",
    "    patch_size = extractor.model.patch_embed.patch_size[0] if DINOV2 else extractor.model.patch_embed.patch_size\n",
    "    num_patches = int(patch_size / stride * (img_size // patch_size - 1) + 1)\n",
    "    \n",
    "    input_text = \"a photo of \"+category[-1][0] if TEXT_INPUT else None\n",
    "\n",
    "    current_save_results = 0\n",
    "\n",
    "    N = len(files) // 2\n",
    "    pbar = tqdm(total=N)\n",
    "    result = []\n",
    "    if 'Anno' in files[0]:\n",
    "        Anno=True\n",
    "    else:\n",
    "        Anno=False\n",
    "    for pair_idx in range(N):\n",
    "\n",
    "        # Load image 1\n",
    "        img1 = Image.open(files[2*pair_idx]).convert('RGB')\n",
    "        img1_input = resize(img1, real_size, resize=True, to_pil=True, edge=EDGE_PAD)\n",
    "        img1 = resize(img1, img_size, resize=True, to_pil=True, edge=EDGE_PAD)\n",
    "\n",
    "        # Load image 2\n",
    "        img2 = Image.open(files[2*pair_idx+1]).convert('RGB')\n",
    "        img2_input = resize(img2, real_size, resize=True, to_pil=True, edge=EDGE_PAD)\n",
    "        img2 = resize(img2, img_size, resize=True, to_pil=True, edge=EDGE_PAD)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if not CO_PCA:\n",
    "                if not ONLY_DINO:\n",
    "                    img1_desc = process_features_and_mask(model, aug, img1_input, input_text=input_text, mask=False, pca=PCA).reshape(1,1,-1, num_patches**2).permute(0,1,3,2)\n",
    "                    img2_desc = process_features_and_mask(model, aug, img2_input, category[-1], input_text=input_text,  mask=mask, pca=PCA).reshape(1,1,-1, num_patches**2).permute(0,1,3,2)\n",
    "                if FUSE_DINO:\n",
    "                    img1_batch = extractor.preprocess_pil(img1)\n",
    "                    img1_desc_dino = extractor.extract_descriptors(img1_batch.to(device), layer, facet)\n",
    "                    img2_batch = extractor.preprocess_pil(img2)\n",
    "                    img2_desc_dino = extractor.extract_descriptors(img2_batch.to(device), layer, facet)\n",
    "\n",
    "            else:\n",
    "                if not ONLY_DINO:\n",
    "                    features1 = process_features_and_mask(model, aug, img1_input, input_text=input_text,  mask=False, raw=True)\n",
    "                    features2 = process_features_and_mask(model, aug, img2_input, input_text=input_text,  mask=False, raw=True)\n",
    "                    processed_features1, processed_features2 = co_pca(features1, features2, PCA_DIMS)\n",
    "                    img1_desc = processed_features1.reshape(1, 1, -1, num_patches**2).permute(0,1,3,2)\n",
    "                    img2_desc = processed_features2.reshape(1, 1, -1, num_patches**2).permute(0,1,3,2)\n",
    "                if FUSE_DINO:\n",
    "                    img1_batch = extractor.preprocess_pil(img1)\n",
    "                    img1_desc_dino = extractor.extract_descriptors(img1_batch.to(device), layer, facet)\n",
    "                    img2_batch = extractor.preprocess_pil(img2)\n",
    "                    img2_desc_dino = extractor.extract_descriptors(img2_batch.to(device), layer, facet)\n",
    "                \n",
    "            if dist == 'l1' or dist == 'l2':\n",
    "                # normalize the features\n",
    "                img1_desc = img1_desc / img1_desc.norm(dim=-1, keepdim=True)\n",
    "                img2_desc = img2_desc / img2_desc.norm(dim=-1, keepdim=True)\n",
    "                if FUSE_DINO:\n",
    "                    img1_desc_dino = img1_desc_dino / img1_desc_dino.norm(dim=-1, keepdim=True)\n",
    "                    img2_desc_dino = img2_desc_dino / img2_desc_dino.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            if FUSE_DINO and not ONLY_DINO:\n",
    "                # cat two features together\n",
    "                img1_desc = torch.cat((img1_desc, img1_desc_dino), dim=-1)\n",
    "                img2_desc = torch.cat((img2_desc, img2_desc_dino), dim=-1)\n",
    "\n",
    "            if ONLY_DINO:\n",
    "                img1_desc = img1_desc_dino\n",
    "                img2_desc = img2_desc_dino\n",
    "\n",
    "            if DRAW_DENSE:\n",
    "                if not Anno:\n",
    "                    mask1 = get_mask(model, aug, img1, category[0])\n",
    "                    mask2 = get_mask(model, aug, img2, category[-1])\n",
    "                if Anno:\n",
    "                    mask1 = torch.Tensor(resize(img1, img_size, resize=True, to_pil=False, edge=EDGE_PAD).mean(-1)>0).to(device)\n",
    "                    mask2 = torch.Tensor(resize(img2, img_size, resize=True, to_pil=False, edge=EDGE_PAD).mean(-1)>0).to(device)\n",
    "                    print(mask1.shape, mask2.shape,mask1.sum(), mask2.sum())\n",
    "                if ONLY_DINO or not FUSE_DINO:\n",
    "                    img1_desc = img1_desc / img1_desc.norm(dim=-1, keepdim=True)\n",
    "                    img2_desc = img2_desc / img2_desc.norm(dim=-1, keepdim=True)\n",
    "                \n",
    "                img1_desc_reshaped = img1_desc.permute(0,1,3,2).reshape(-1, img1_desc.shape[-1], num_patches, num_patches)\n",
    "                img2_desc_reshaped = img2_desc.permute(0,1,3,2).reshape(-1, img2_desc.shape[-1], num_patches, num_patches)\n",
    "                trg_dense_output, src_color_map = find_nearest_patchs(mask2, mask1, img2, img1, img2_desc_reshaped, img1_desc_reshaped, mask=mask)\n",
    "\n",
    "                if not os.path.exists(f'{save_path}/{category[0]}'):\n",
    "                    os.makedirs(f'{save_path}/{category[0]}')\n",
    "                fig_colormap, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                ax1.axis('off')\n",
    "                ax2.axis('off')\n",
    "                ax1.imshow(src_color_map)\n",
    "                ax2.imshow(trg_dense_output)\n",
    "                fig_colormap.savefig(f'{save_path}/{category[0]}/{pair_idx}_colormap.png')\n",
    "                plt.close(fig_colormap)\n",
    "            \n",
    "            if DRAW_SWAP:\n",
    "                if not DRAW_DENSE:\n",
    "                    mask1 = get_mask(model, aug, img1, category[0])\n",
    "                    mask2 = get_mask(model, aug, img2, category[-1])\n",
    "\n",
    "                if (ONLY_DINO or not FUSE_DINO) and not DRAW_DENSE:\n",
    "                    img1_desc = img1_desc / img1_desc.norm(dim=-1, keepdim=True)\n",
    "                    img2_desc = img2_desc / img2_desc.norm(dim=-1, keepdim=True)\n",
    "                    \n",
    "                img1_desc_reshaped = img1_desc.permute(0,1,3,2).reshape(-1, img1_desc.shape[-1], num_patches, num_patches)\n",
    "                img2_desc_reshaped = img2_desc.permute(0,1,3,2).reshape(-1, img2_desc.shape[-1], num_patches, num_patches)\n",
    "                trg_dense_output, src_color_map = find_nearest_patchs_replace(mask2, mask1, img2, img1, img2_desc_reshaped, img1_desc_reshaped, mask=mask, resolution=156)\n",
    "                if not os.path.exists(f'{save_path}/{category[0]}'):\n",
    "                    os.makedirs(f'{save_path}/{category[0]}')\n",
    "                fig_colormap, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "                ax1.axis('off')\n",
    "                ax2.axis('off')\n",
    "                ax1.imshow(src_color_map)\n",
    "                ax2.imshow(trg_dense_output)\n",
    "                fig_colormap.savefig(f'{save_path}/{category[0]}/{pair_idx}_swap.png')\n",
    "                plt.close(fig_colormap)\n",
    "            if not DRAW_SWAP and not DRAW_DENSE:\n",
    "                result.append([img1_desc.cpu(), img2_desc.cpu()])\n",
    "            else:\n",
    "                result.append([img1_desc.cpu(), img2_desc.cpu(), mask1.cpu(), mask2.cpu()])\n",
    "\n",
    "    pbar.update(1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis_pca_mask(result,save_path):\n",
    "    # PCA visualization mask version\n",
    "    for (feature1,feature2,mask1,mask2) in result:\n",
    "        # feature1 shape (1,1,3600,768*2)\n",
    "        # feature2 shape (1,1,3600,768*2)\n",
    "        num_patches = int(math.sqrt(feature1.shape[-2]))\n",
    "        # pca the concatenated feature to 3 dimensions\n",
    "        feature1 = feature1.squeeze() # shape (3600,768*2)\n",
    "        feature2 = feature2.squeeze() # shape (3600,768*2)\n",
    "        chennel_dim = feature1.shape[-1]\n",
    "        # resize back\n",
    "        src_feature_reshaped = feature1.squeeze().permute(1,0).reshape(-1,num_patches,num_patches).cuda()\n",
    "        tgt_feature_reshaped = feature2.squeeze().permute(1,0).reshape(-1,num_patches,num_patches).cuda()\n",
    "        resized_src_mask = F.interpolate(mask1.unsqueeze(0).unsqueeze(0), size=(num_patches, num_patches), mode='nearest').squeeze().cuda()\n",
    "        resized_tgt_mask = F.interpolate(mask2.unsqueeze(0).unsqueeze(0), size=(num_patches, num_patches), mode='nearest').squeeze().cuda()\n",
    "        src_feature_upsampled = src_feature_reshaped * resized_src_mask.repeat(src_feature_reshaped.shape[0],1,1)\n",
    "        tgt_feature_upsampled = tgt_feature_reshaped * resized_tgt_mask.repeat(src_feature_reshaped.shape[0],1,1)\n",
    "        feature1=src_feature_upsampled.reshape(chennel_dim,-1).permute(1,0)\n",
    "        feature2=tgt_feature_upsampled.reshape(chennel_dim,-1).permute(1,0)\n",
    "\n",
    "        n_components=4 # the first component is to seperate the object from the background\n",
    "        pca = sklearnPCA(n_components=n_components)\n",
    "        feature1_n_feature2 = torch.cat((feature1,feature2),dim=0) # shape (7200,768*2)\n",
    "        feature1_n_feature2 = pca.fit_transform(feature1_n_feature2.cpu().numpy()) # shape (7200,3)\n",
    "        feature1 = feature1_n_feature2[:feature1.shape[0],:] # shape (3600,3)\n",
    "        feature2 = feature1_n_feature2[feature1.shape[0]:,:] # shape (3600,3)\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(4, 2, figsize=(10, 14))\n",
    "        for show_channel in range(n_components):\n",
    "            if show_channel==0:\n",
    "                continue\n",
    "            # min max normalize the feature map\n",
    "            feature1[:, show_channel] = (feature1[:, show_channel] - feature1[:, show_channel].min()) / (feature1[:, show_channel].max() - feature1[:, show_channel].min())\n",
    "            feature2[:, show_channel] = (feature2[:, show_channel] - feature2[:, show_channel].min()) / (feature2[:, show_channel].max() - feature2[:, show_channel].min())\n",
    "            feature1_first_channel = feature1[:, show_channel].reshape(num_patches,num_patches)\n",
    "            feature2_first_channel = feature2[:, show_channel].reshape(num_patches,num_patches)\n",
    "\n",
    "            axes[show_channel-1, 0].imshow(feature1_first_channel)\n",
    "            axes[show_channel-1, 0].axis('off')\n",
    "            axes[show_channel-1, 1].imshow(feature2_first_channel)\n",
    "            axes[show_channel-1, 1].axis('off')\n",
    "            axes[show_channel-1, 0].set_title('Feature 1 - Channel {}'.format(show_channel ), fontsize=14)\n",
    "            axes[show_channel-1, 1].set_title('Feature 2 - Channel {}'.format(show_channel ), fontsize=14)\n",
    "\n",
    "\n",
    "        feature1_resized = feature1[:, 1:4].reshape(num_patches,num_patches, 3)\n",
    "        feature2_resized = feature2[:, 1:4].reshape(num_patches,num_patches, 3)\n",
    "\n",
    "        axes[3, 0].imshow(feature1_resized)\n",
    "        axes[3, 0].axis('off')\n",
    "        axes[3, 1].imshow(feature2_resized)\n",
    "        axes[3, 1].axis('off')\n",
    "        axes[3, 0].set_title('Feature 1 - All Channels', fontsize=14)\n",
    "        axes[3, 1].set_title('Feature 2 - All Channels', fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(save_path+'/masked_pca.png', dpi=300)\n",
    "\n",
    "def vis_pca(result,save_path,src_img_path,trg_img_path):\n",
    "    # PCA visualization\n",
    "    for (feature1,feature2,mask1,mask2) in result:\n",
    "        # feature1 shape (1,1,3600,768*2)\n",
    "        # feature2 shape (1,1,3600,768*2)\n",
    "        num_patches=int(math.sqrt(feature1.shape[2]))\n",
    "        # pca the concatenated feature to 3 dimensions\n",
    "        feature1 = feature1.squeeze() # shape (3600,768*2)\n",
    "        feature2 = feature2.squeeze() # shape (3600,768*2)\n",
    "        chennel_dim = feature1.shape[-1]\n",
    "        # resize back\n",
    "        h1, w1 = Image.open(src_img_path).size\n",
    "        scale_h1 = h1/num_patches\n",
    "        scale_w1 = w1/num_patches\n",
    "        \n",
    "        if scale_h1 > scale_w1:\n",
    "            scale = scale_h1\n",
    "            scaled_w = int(w1/scale)\n",
    "            feature1 = feature1.reshape(num_patches,num_patches,chennel_dim)\n",
    "            feature1_uncropped=feature1[(num_patches-scaled_w)//2:num_patches-(num_patches-scaled_w)//2,:,:]\n",
    "        else:\n",
    "            scale = scale_w1\n",
    "            scaled_h = int(h1/scale)\n",
    "            feature1 = feature1.reshape(num_patches,num_patches,chennel_dim)\n",
    "            feature1_uncropped=feature1[:,(num_patches-scaled_h)//2:num_patches-(num_patches-scaled_h)//2,:]\n",
    "        \n",
    "        h2, w2 = Image.open(trg_img_path).size\n",
    "        scale_h2 = h2/num_patches\n",
    "        scale_w2 = w2/num_patches\n",
    "        if scale_h2 > scale_w2:\n",
    "            scale = scale_h2\n",
    "            scaled_w = int(w2/scale)\n",
    "            feature2 = feature2.reshape(num_patches,num_patches,chennel_dim)\n",
    "            feature2_uncropped=feature2[(num_patches-scaled_w)//2:num_patches-(num_patches-scaled_w)//2,:,:]\n",
    "        else:\n",
    "            scale = scale_w2\n",
    "            scaled_h = int(h2/scale)\n",
    "            feature2 = feature2.reshape(num_patches,num_patches,chennel_dim)\n",
    "            feature2_uncropped=feature2[:,(num_patches-scaled_h)//2:num_patches-(num_patches-scaled_h)//2,:]\n",
    "\n",
    "        f1_shape=feature1_uncropped.shape[:2]\n",
    "        f2_shape=feature2_uncropped.shape[:2]\n",
    "        feature1 = feature1_uncropped.reshape(f1_shape[0]*f1_shape[1],chennel_dim)\n",
    "        feature2 = feature2_uncropped.reshape(f2_shape[0]*f2_shape[1],chennel_dim)\n",
    "        n_components=3\n",
    "        pca = sklearnPCA(n_components=n_components)\n",
    "        feature1_n_feature2 = torch.cat((feature1,feature2),dim=0) # shape (7200,768*2)\n",
    "        feature1_n_feature2 = pca.fit_transform(feature1_n_feature2.cpu().numpy()) # shape (7200,3)\n",
    "        feature1 = feature1_n_feature2[:feature1.shape[0],:] # shape (3600,3)\n",
    "        feature2 = feature1_n_feature2[feature1.shape[0]:,:] # shape (3600,3)\n",
    "        \n",
    "        \n",
    "        fig, axes = plt.subplots(4, 2, figsize=(10, 14))\n",
    "        for show_channel in range(n_components):\n",
    "            # min max normalize the feature map\n",
    "            feature1[:, show_channel] = (feature1[:, show_channel] - feature1[:, show_channel].min()) / (feature1[:, show_channel].max() - feature1[:, show_channel].min())\n",
    "            feature2[:, show_channel] = (feature2[:, show_channel] - feature2[:, show_channel].min()) / (feature2[:, show_channel].max() - feature2[:, show_channel].min())\n",
    "            feature1_first_channel = feature1[:, show_channel].reshape(f1_shape[0], f1_shape[1])\n",
    "            feature2_first_channel = feature2[:, show_channel].reshape(f2_shape[0], f2_shape[1])\n",
    "\n",
    "            axes[show_channel, 0].imshow(feature1_first_channel)\n",
    "            axes[show_channel, 0].axis('off')\n",
    "            axes[show_channel, 1].imshow(feature2_first_channel)\n",
    "            axes[show_channel, 1].axis('off')\n",
    "            axes[show_channel, 0].set_title('Feature 1 - Channel {}'.format(show_channel + 1), fontsize=14)\n",
    "            axes[show_channel, 1].set_title('Feature 2 - Channel {}'.format(show_channel + 1), fontsize=14)\n",
    "\n",
    "\n",
    "        feature1_resized = feature1[:, :3].reshape(f1_shape[0], f1_shape[1], 3)\n",
    "        feature2_resized = feature2[:, :3].reshape(f2_shape[0], f2_shape[1], 3)\n",
    "\n",
    "        axes[3, 0].imshow(feature1_resized)\n",
    "        axes[3, 0].axis('off')\n",
    "        axes[3, 1].imshow(feature2_resized)\n",
    "        axes[3, 1].axis('off')\n",
    "        axes[3, 0].set_title('Feature 1 - All Channels', fontsize=14)\n",
    "        axes[3, 1].set_title('Feature 2 - All Channels', fontsize=14)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(save_path+'/pca.png', dpi=300)\n",
    "\n",
    "\n",
    "def perform_clustering(features, n_clusters=10):\n",
    "    # Normalize features\n",
    "    features = F.normalize(features, p=2, dim=1)\n",
    "    # Convert the features to float32\n",
    "    features = features.cpu().detach().numpy().astype('float32')\n",
    "    # Initialize a k-means clustering index with the desired number of clusters\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    # Train the k-means index with the features\n",
    "    kmeans.fit(features)\n",
    "    # Assign the features to their nearest cluster\n",
    "    labels = kmeans.predict(features)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def cluster_and_match(result, save_path, n_clusters=6):\n",
    "    for (feature1,feature2,mask1,mask2) in result:\n",
    "        # feature1 shape (1,1,3600,768*2)\n",
    "        num_patches = int(math.sqrt(feature1.shape[-2]))\n",
    "        # pca the concatenated feature to 3 dimensions\n",
    "        feature1 = feature1.squeeze() # shape (3600,768*2)\n",
    "        feature2 = feature2.squeeze() # shape (3600,768*2)\n",
    "        chennel_dim = feature1.shape[-1]\n",
    "        # resize back\n",
    "\n",
    "        src_feature_reshaped = feature1.squeeze().permute(1,0).reshape(-1,num_patches,num_patches).cuda()\n",
    "        tgt_feature_reshaped = feature2.squeeze().permute(1,0).reshape(-1,num_patches,num_patches).cuda()\n",
    "        resized_src_mask = F.interpolate(mask1.unsqueeze(0).unsqueeze(0), size=(num_patches, num_patches), mode='nearest').squeeze().cuda()\n",
    "        resized_tgt_mask = F.interpolate(mask2.unsqueeze(0).unsqueeze(0), size=(num_patches, num_patches), mode='nearest').squeeze().cuda()\n",
    "        src_feature_upsampled = src_feature_reshaped * resized_src_mask.repeat(src_feature_reshaped.shape[0],1,1)\n",
    "        tgt_feature_upsampled = tgt_feature_reshaped * resized_tgt_mask.repeat(src_feature_reshaped.shape[0],1,1)\n",
    "\n",
    "        feature1=src_feature_upsampled.unsqueeze(0)\n",
    "        feature2=tgt_feature_upsampled.unsqueeze(0)\n",
    "        \n",
    "        w1, h1 = feature1.shape[2], feature1.shape[3]\n",
    "        w2, h2 = feature2.shape[2], feature2.shape[3]\n",
    "\n",
    "        features1_2d = feature1.reshape(feature1.shape[1], -1).permute(1, 0)\n",
    "        features2_2d = feature2.reshape(feature2.shape[1], -1).permute(1, 0)\n",
    "\n",
    "        labels_img1 = perform_clustering(features1_2d, n_clusters)\n",
    "        labels_img2 = perform_clustering(features2_2d, n_clusters)\n",
    "\n",
    "        cluster_means_img1 = [features1_2d.cpu().detach().numpy()[labels_img1 == i].mean(axis=0) for i in range(n_clusters)]\n",
    "        cluster_means_img2 = [features2_2d.cpu().detach().numpy()[labels_img2 == i].mean(axis=0) for i in range(n_clusters)]\n",
    "\n",
    "        distances = np.linalg.norm(np.expand_dims(cluster_means_img1, axis=1) - np.expand_dims(cluster_means_img2, axis=0), axis=-1)\n",
    "        # Use Hungarian algorithm to find the optimal bijective mapping\n",
    "        row_ind, col_ind = linear_sum_assignment(distances)\n",
    "\n",
    "        relabeled_img2 = np.zeros_like(labels_img2)\n",
    "        for i, match in zip(row_ind, col_ind):\n",
    "            relabeled_img2[labels_img2 == match] = i\n",
    "\n",
    "        labels_img1 = labels_img1.reshape(w1, h1)\n",
    "        relabeled_img2 = relabeled_img2.reshape(w2, h2)\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        # Plot the results\n",
    "        ax_img1 = axs[0]\n",
    "        axs[0].axis('off')\n",
    "        ax_img1.imshow(labels_img1, cmap='tab20')\n",
    "        \n",
    "        ax_img2 = axs[1]\n",
    "        axs[1].axis('off')\n",
    "        ax_img2.imshow(relabeled_img2, cmap='tab20')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        fig.savefig(save_path+'/clustering.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(src_img_path,trg_img_path):\n",
    "\n",
    "    categories = [['dog'], ['dog']]\n",
    "    files = [src_img_path, trg_img_path]\n",
    "    save_path = './results_vis' + f'/{trg_img_path.split(\"/\")[-1].split(\".\")[0]}_{src_img_path.split(\"/\")[-1].split(\".\")[0]}'\n",
    "    result = compute_pair_feature(model, aug, save_path, files, mask=MASK, category=categories, dist=DIST)\n",
    "    if MASK:\n",
    "        vis_pca_mask(result, save_path)\n",
    "        cluster_and_match(result, save_path)\n",
    "    if 'Anno' not in src_img_path:\n",
    "        vis_pca(result, save_path,src_img_path,trg_img_path)\n",
    "\n",
    "    return result\n",
    "\n",
    "src_img_path = \"data/images/dog_00.jpg\"\n",
    "trg_img_path = \"data/images/dog_59.jpg\"\n",
    "result = process_images(src_img_path, trg_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_img_path = \"data/images/dog_Anno_00.png\"\n",
    "trg_img_path = \"data/images/dog_Anno_59.png\"\n",
    "result = process_images(src_img_path, trg_img_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "colossal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
